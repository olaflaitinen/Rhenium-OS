# Copyright (c) 2025 Skolyn LLC. All rights reserved.

"""
Fairness Reports Module
=======================

Generation of comprehensive fairness evaluation reports in human-readable
and machine-readable formats.

"""

from __future__ import annotations

import json
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from rhenium.governance.fairness_metrics import FairnessMetrics, SubgroupMetrics
from rhenium.core.logging import get_governance_logger

logger = get_governance_logger()


def generate_fairness_report(
    fairness_metrics: FairnessMetrics,
    pipeline_name: str,
    dataset_name: str,
    output_path: Path | None = None,
    format: str = "markdown",
) -> str:
    """
    Generate a fairness evaluation report.
    
    Args:
        fairness_metrics: Computed fairness metrics
        pipeline_name: Name of evaluated pipeline
        dataset_name: Name of evaluation dataset
        output_path: Optional path to save report
        format: Output format (markdown, json)
        
    Returns:
        Report content as string
    """
    if format == "markdown":
        content = _generate_markdown_report(fairness_metrics, pipeline_name, dataset_name)
    elif format == "json":
        content = _generate_json_report(fairness_metrics, pipeline_name, dataset_name)
    else:
        raise ValueError(f"Unsupported format: {format}")
        
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(content)
        logger.info("Fairness report saved", path=str(output_path))
        
    return content


def _generate_markdown_report(
    metrics: FairnessMetrics,
    pipeline_name: str,
    dataset_name: str,
) -> str:
    """Generate markdown fairness report."""
    lines = [
        "# Fairness Evaluation Report",
        "",
        f"**Pipeline**: {pipeline_name}",
        f"**Dataset**: {dataset_name}",
        f"**Generated**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}",
        "",
        "---",
        "",
        "## Overall Performance",
        "",
    ]
    
    for metric, value in metrics.overall_metrics.items():
        if isinstance(value, float):
            lines.append(f"- **{metric}**: {value:.4f}")
        else:
            lines.append(f"- **{metric}**: {value}")
    
    lines.extend([
        "",
        "---",
        "",
        "## Subgroup Performance",
        "",
        "| Dimension | Subgroup | N | AUC | Calibration Error |",
        "|-----------|----------|---|-----|-------------------|",
    ])
    
    for sg in metrics.subgroup_metrics:
        auc_str = f"{sg.auc:.4f}" if sg.auc else "N/A"
        cal_str = f"{sg.calibration_error:.4f}" if sg.calibration_error else "N/A"
        lines.append(f"| {sg.subgroup_name} | {sg.subgroup_value} | {sg.sample_count} | {auc_str} | {cal_str} |")
    
    lines.extend([
        "",
        "---",
        "",
        "## Disparity Analysis",
        "",
    ])
    
    for metric, value in metrics.disparity_metrics.items():
        lines.append(f"- **{metric}**: {value:.4f}")
    
    lines.extend([
        "",
        "---",
        "",
        "## Interpretation",
        "",
        "This report presents performance metrics stratified by demographic and institutional factors. ",
        "Large disparities (max_disparity > 0.05 or disparity_ratio < 0.95) may indicate algorithmic bias ",
        "requiring investigation and potential mitigation.",
        "",
        "---",
        "",
        "*Report generated by Skolyn Rhenium OS Fairness Evaluation Framework*",
    ])
    
    return "\n".join(lines)


def _generate_json_report(
    metrics: FairnessMetrics,
    pipeline_name: str,
    dataset_name: str,
) -> str:
    """Generate JSON fairness report."""
    report = {
        "report_type": "fairness_evaluation",
        "pipeline_name": pipeline_name,
        "dataset_name": dataset_name,
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "overall_metrics": metrics.overall_metrics,
        "subgroup_metrics": [asdict(sg) for sg in metrics.subgroup_metrics],
        "disparity_metrics": metrics.disparity_metrics,
    }
    return json.dumps(report, indent=2, default=str)


def assess_fairness_thresholds(
    metrics: FairnessMetrics,
    max_disparity_threshold: float = 0.05,
    min_ratio_threshold: float = 0.95,
) -> dict[str, Any]:
    """
    Assess whether fairness metrics meet acceptable thresholds.
    
    Returns:
        Assessment results with pass/fail status and details
    """
    issues = []
    
    for key, value in metrics.disparity_metrics.items():
        if "max_disparity" in key and value > max_disparity_threshold:
            issues.append({
                "metric": key,
                "value": value,
                "threshold": max_disparity_threshold,
                "issue": "Max disparity exceeds threshold",
            })
        if "disparity_ratio" in key and value < min_ratio_threshold:
            issues.append({
                "metric": key,
                "value": value,
                "threshold": min_ratio_threshold,
                "issue": "Disparity ratio below threshold",
            })
    
    return {
        "passed": len(issues) == 0,
        "issues": issues,
        "thresholds": {
            "max_disparity": max_disparity_threshold,
            "min_ratio": min_ratio_threshold,
        },
    }
